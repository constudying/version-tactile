# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
"""
DETR Transformer class.

Copy-paste from torch.nn.Transformer with modifications:
    * positional encodings are passed in MHattention
    * extra LN at the end of encoder is removed
    * decoder returns a stack of activations from all decoding layers

å®šä¹‰äº†ä¸€ä¸ªåä¸º DETR Transformer çš„æ¨¡å—ï¼Œä¸»è¦ç”¨äºå®ç°åŸºäº Transformer çš„æ·±åº¦å­¦ä¹ æ¨¡å‹
ï¼ˆå¦‚ DETR: End-to-End Object Detection with Transformersï¼‰ã€‚
å®ƒæ˜¯å¯¹ PyTorch è‡ªå¸¦çš„ torch.nn.Transformer ç±»çš„ä¿®æ”¹ç‰ˆæœ¬ï¼Œé’ˆå¯¹ç›®æ ‡æ£€æµ‹ä»»åŠ¡è¿›è¡Œäº†ä¼˜åŒ–ã€‚
"""
import copy
from typing import Optional, List

import torch
import torch.nn.functional as F
from torch import nn, Tensor

import IPython
e = IPython.embed

"""
è¿™ä¸ª Transformer ç±»æ˜¯ä¸€ä¸ªå®šåˆ¶çš„ Transformer æ¨¡å—ï¼Œ
åŒ…å« Encoder å’Œ Decoder ä¸¤éƒ¨åˆ†ã€‚è¯¥æ¨¡å—ç”¨äºå¤„ç†åºåˆ—åŒ–ç‰¹å¾æ•°æ®ï¼Œ
å¹¶ä¸ºä»»åŠ¡ï¼ˆå¦‚ç›®æ ‡æ£€æµ‹ã€ç‰¹å¾è§£ç ç­‰ï¼‰æä¾›é«˜çº§ç‰¹å¾è¾“å‡ºã€‚
"""
class Transformer(nn.Module):

    """
    d_model: ç‰¹å¾åµŒå…¥çš„ç»´åº¦ï¼ˆé»˜è®¤ 512ï¼‰ã€‚æ‰€æœ‰è¾“å…¥å’Œè¾“å‡ºçš„ç‰¹å¾ç»´åº¦å¿…é¡»ä¸ä¹‹åŒ¹é…
    nhead: å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ä¸­çš„å¤´æ•°
    num_encoder_layers / num_decoder_layers: Encoder å’Œ Decoder å †å çš„å±‚æ•°
    dim_feedforwardï¼šå‰é¦ˆç½‘ç»œï¼ˆFFNï¼‰ä¸­çš„éšè—å±‚ç»´åº¦ã€‚
    dropoutï¼šDropout æ¦‚ç‡ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆã€‚
    activationï¼šæ¿€æ´»å‡½æ•°ç±»å‹ï¼ˆé»˜è®¤ reluï¼‰ã€‚
    normalize_beforeï¼šæ˜¯å¦åœ¨å±‚å‰æ‰§è¡Œ LayerNormã€‚
    return_intermediate_decï¼šæ˜¯å¦è¿”å› Decoder æ¯ä¸€å±‚çš„ä¸­é—´ç»“æœã€‚
    """
    """
    å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ï¼ˆMulti-Head Attention, MHAï¼‰ æ˜¯ Transformer æ¨¡å‹ä¸­çš„æ ¸å¿ƒç»„ä»¶ï¼Œ
    ç”¨äºåœ¨åºåˆ—æ•°æ®ï¼ˆå¦‚æ–‡æœ¬ã€å›¾åƒç‰¹å¾ç­‰ï¼‰ä¸­å»ºæ¨¡ä¸åŒä½ç½®ä¹‹é—´çš„å…³ç³»ã€‚
    ç›¸æ¯”å•å¤´æ³¨æ„åŠ›æœºåˆ¶ï¼Œå¤šå¤´æ³¨æ„åŠ›èƒ½å¤Ÿæ›´å¥½åœ°æ•è·è¾“å…¥æ•°æ®çš„å¤šç§ç‰¹å¾æ¨¡å¼ã€‚
    æ³¨æ„åŠ›æœºåˆ¶çš„æ ¸å¿ƒæ€æƒ³æ˜¯ï¼šå¯¹äºæ¯ä¸ªè¾“å…¥ä½ç½®ï¼Œè®¡ç®—å®ƒä¸å…¶ä»–ä½ç½®çš„ç›¸å…³æ€§åˆ†æ•°ï¼Œ
    å¹¶æ ¹æ®è¿™äº›åˆ†æ•°å¯¹å…¶ä»–ä½ç½®çš„ä¿¡æ¯è¿›è¡ŒåŠ æƒæ±‡æ€»ã€‚
    """
    """
    Dropout æ˜¯ä¸€ç§å¸¸ç”¨çš„æ­£åˆ™åŒ–æŠ€æœ¯ï¼Œä¸»è¦ç”¨äºé˜²æ­¢æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å‘ç”Ÿè¿‡æ‹Ÿåˆï¼ˆoverfittingï¼‰ã€‚
    å®ƒé€šè¿‡éšæœºåœ°å°†ç¥ç»ç½‘ç»œä¸­çš„ä¸€éƒ¨åˆ†ç¥ç»å…ƒâ€œå…³é—­â€ï¼ˆå³å°†å®ƒä»¬çš„è¾“å‡ºç½®ä¸º 0ï¼‰ï¼Œ
    æ¥å‡å°‘ç¥ç»å…ƒä¹‹é—´çš„ç›¸äº’ä¾èµ–ï¼Œå¢å¼ºæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚
    """
    # åˆå§‹åŒ–ä¸€ä¸ªå®Œæ•´çš„ Transformer æ¨¡å—ï¼ŒåŒ…æ‹¬ Encoder å’Œ Decoder ä¸¤éƒ¨åˆ†ï¼Œä»¥åŠå¿…è¦çš„é…ç½®å’Œå‚æ•°åˆå§‹åŒ–ã€‚
    def __init__(self, d_model=512, nhead=8, num_encoder_layers=6,
                 num_decoder_layers=6, dim_feedforward=2048, dropout=0.1,
                 activation="relu", normalize_before=False,
                 return_intermediate_dec=False):
        super().__init__() # è°ƒç”¨çˆ¶ç±» nn.Module çš„åˆå§‹åŒ–æ–¹æ³•ï¼Œç¡®ä¿å­ç±»èƒ½å¤Ÿæ­£ç¡®ç»§æ‰¿çˆ¶ç±»çš„åŠŸèƒ½ã€‚
        """
        TransformerEncoderLayerï¼šå®šä¹‰äº†å•ä¸ª Encoder å±‚ã€‚
        åŒ…æ‹¬ï¼š
            å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶ã€‚
            å‰é¦ˆç½‘ç»œã€‚
            å½’ä¸€åŒ–å’Œ Dropoutã€‚
        """
        encoder_layer = TransformerEncoderLayer(d_model, nhead, dim_feedforward,
                                                dropout, activation, normalize_before)
        encoder_norm = nn.LayerNorm(d_model) if normalize_before else None  # å±‚å½’ä¸€åŒ–é€šè¿‡å¯¹æ¯ä¸€å±‚çš„è¾“å‡ºè¿›è¡Œå½’ä¸€åŒ–ï¼Œä½¿å¾—æ¿€æ´»å€¼å…·æœ‰å‡åŒ€çš„åˆ†å¸ƒï¼Œä»è€Œç¨³å®šæ¢¯åº¦æ›´æ–°ã€‚
        self.encoder = TransformerEncoder(encoder_layer, num_encoder_layers, encoder_norm)
        

        """
        TransformerDecoderLayerï¼šå®šä¹‰äº†å•ä¸ª Decoder å±‚ã€‚
                åŒ…æ‹¬ï¼š
                    ç›®æ ‡æŸ¥è¯¢çš„å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶ã€‚
                    è·¨æ³¨æ„åŠ›æœºåˆ¶ï¼ˆç»“åˆ Encoder çš„è¾“å‡ºï¼‰ã€‚
                    å‰é¦ˆç½‘ç»œã€‚
                    å½’ä¸€åŒ–å’Œ Dropoutã€‚
        """
        decoder_layer = TransformerDecoderLayer(d_model, nhead, dim_feedforward,
                                                dropout, activation, normalize_before)
        decoder_norm = nn.LayerNorm(d_model)
        self.decoder = TransformerDecoder(decoder_layer, num_decoder_layers, decoder_norm,
                                          return_intermediate=return_intermediate_dec)

        self._reset_parameters() # é‡ç½®æ¨¡å‹ä¸­çš„å‚æ•°ï¼Œç¡®ä¿è®­ç»ƒå¼€å§‹æ—¶æƒé‡åˆå§‹åŒ–å¾—å½“ã€‚
        # å­˜å‚¨æ¨¡å‹çš„æ ¸å¿ƒè¶…å‚æ•°,å­˜å‚¨ d_model å’Œ nheadï¼Œåœ¨æ¨¡å‹å‰å‘ä¼ æ’­å’Œå…¶ä»–è®¡ç®—ä¸­ä¼šç”¨åˆ°ã€‚
        self.d_model = d_model
        self.nhead = nhead
    # ä½¿ç”¨ Xavier å‡åŒ€åˆ†å¸ƒåˆå§‹åŒ–æƒé‡ï¼Œç¡®ä¿æ¨¡å‹æ•°å€¼ç¨³å®šæ€§ã€‚
    def _reset_parameters(self):
        for p in self.parameters():
            if p.dim() > 1:
                nn.init.xavier_uniform_(p)

    """
    æ€»ç»“
        åŠŸèƒ½
            è¾“å…¥ç‰¹å¾é¢„å¤„ç†ï¼š
            æ ¹æ®è¾“å…¥å½¢çŠ¶ï¼Œå±•å¹³å’Œè°ƒæ•´ç‰¹å¾ã€‚
            æ·»åŠ ä½ç½®ç¼–ç å’Œé¢å¤–ç‰¹å¾ã€‚
        Encoderï¼š
            æå–è¾“å…¥ç‰¹å¾çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚
        Decoderï¼š
            ä½¿ç”¨ç›®æ ‡æŸ¥è¯¢åµŒå…¥ç”Ÿæˆè§£ç ç‰¹å¾ã€‚
        é€‚ç”¨åœºæ™¯
            ç›®æ ‡æ£€æµ‹ï¼š
                query_embed è¡¨ç¤ºç›®æ ‡æŸ¥è¯¢ï¼Œç”¨äºç”Ÿæˆæ£€æµ‹æ¡†å’Œç±»åˆ«ã€‚
            åºåˆ—åˆ°åºåˆ—ä»»åŠ¡ï¼š
                å¦‚ç¿»è¯‘ã€ç‰¹å¾è§£ç ç­‰ã€‚
    """
    # æ¥æ”¶è¾“å…¥ç‰¹å¾ã€æ©ç å’ŒåµŒå…¥ä¿¡æ¯ï¼Œç»è¿‡ Encoder å’Œ Decoder å¤„ç†åï¼Œç”Ÿæˆè§£ç çš„ç‰¹å¾è¾“å‡ºã€‚
    def forward(self, src, mask, query_embed, pos_embed, latent_input=None, proprio_input=None, additional_pos_embed=None):
        # TODO flatten only when input has H and W
        # è¾“å…¥å½¢çŠ¶;å±•å¹³ä¸è°ƒæ•´;ä½ç½®ç¼–ç è°ƒæ•´;ç›®æ ‡æŸ¥è¯¢åµŒå…¥è°ƒæ•´
        if len(src.shape) == 4: # has H and W
            # flatten NxCxHxW to HWxNxC
            bs, c, h, w = src.shape
            src = src.flatten(2).permute(2, 0, 1)
            pos_embed = pos_embed.flatten(2).permute(2, 0, 1).repeat(1, bs, 1)
            query_embed = query_embed.unsqueeze(1).repeat(1, bs, 1)
            # mask = mask.flatten(1)

            # æ·»åŠ é¢å¤–ç‰¹å¾å’Œä½ç½®ç¼–ç 
            # é¢å¤–ä½ç½®ç¼–ç ï¼šå°† additional_pos_embed æ·»åŠ åˆ°ä½ç½®ç¼–ç ä¸­ï¼Œæ‰©å±•ä½ç½®ä¿¡æ¯çš„èŒƒå›´ã€‚
            # é¢å¤–ç‰¹å¾è¾“å…¥ï¼šå †å  latent_input å’Œ proprio_inputï¼Œå¹¶ä¸ src ç»„åˆ
            additional_pos_embed = additional_pos_embed.unsqueeze(1).repeat(1, bs, 1) # seq, bs, dim
            pos_embed = torch.cat([additional_pos_embed, pos_embed], axis=0)

            addition_input = torch.stack([latent_input, proprio_input], axis=0)
            src = torch.cat([addition_input, src], axis=0)
        else:
            assert len(src.shape) == 3
            # flatten NxHWxC to HWxNxC
            bs, hw, c = src.shape
            src = src.permute(1, 0, 2)
            pos_embed = pos_embed.unsqueeze(1).repeat(1, bs, 1)
            query_embed = query_embed.unsqueeze(1).repeat(1, bs, 1)

        tgt = torch.zeros_like(query_embed) # Decoder çš„ç›®æ ‡è¾“å…¥å¼ é‡ï¼ˆtgtï¼‰åˆå§‹åŒ–ä¸ºå…¨é›¶ï¼Œå½¢çŠ¶ä¸ query_embed ä¸€è‡´ã€‚
        # æ¥æ”¶è¾“å…¥ç‰¹å¾ src å’Œä½ç½®ç¼–ç  pos_embedã€‚è¾“å‡ºè®°å¿†å¼ é‡ memoryï¼ŒåŒ…å«è¾“å…¥ç‰¹å¾çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚
        memory = self.encoder(src, src_key_padding_mask=mask, pos=pos_embed)
        # self.decoderï¼šæ¥æ”¶ç›®æ ‡å¼ é‡ tgt å’Œ memoryï¼ˆEncoder çš„è¾“å‡ºï¼‰ã€‚ä½¿ç”¨ç›®æ ‡æŸ¥è¯¢åµŒå…¥ query_embed å¼•å¯¼è§£ç ã€‚è¿”å›ç›®æ ‡ç‰¹å¾â„ğ‘ 
        hs = self.decoder(tgt, memory, memory_key_padding_mask=mask,
                          pos=pos_embed, query_pos=query_embed)
        hs = hs.transpose(1, 2) # è°ƒæ•´ç›®æ ‡ç‰¹å¾çš„ç»´åº¦ä»¥åŒ¹é…é¢„æœŸæ ¼å¼ã€‚
        return hs


"""
è¿™ä¸ª TransformerEncoder ç±»æ˜¯ Transformer æ¨¡å‹çš„ç¼–ç å™¨æ¨¡å—ã€‚
å®ƒå°†è¾“å…¥ç‰¹å¾é€šè¿‡å¤šå±‚ TransformerEncoderLayer è¿›è¡Œå¤„ç†ï¼Œç”Ÿæˆä¸Šä¸‹æ–‡å¢å¼ºçš„ç‰¹å¾è¡¨ç¤ºã€‚
å·¥ä½œæµç¨‹æ€»ç»“
    è¾“å…¥ç‰¹å¾ï¼š
        è¾“å…¥ç‰¹å¾åºåˆ— src ä¾æ¬¡é€šè¿‡å¤šä¸ª TransformerEncoderLayerã€‚
        æ¯ä¸€å±‚éƒ½ä¼šå¢å¼ºç‰¹å¾çš„ä¸Šä¸‹æ–‡è¡¨ç¤ºã€‚
    é€å±‚å †å ï¼š
        æ¯å±‚çš„è¾“å‡ºä½œä¸ºä¸‹ä¸€å±‚çš„è¾“å…¥ï¼Œæ•è·åºåˆ—ä¸­æ›´é«˜å±‚æ¬¡çš„ä¾èµ–å…³ç³»ã€‚
    æœ€ç»ˆå½’ä¸€åŒ–ï¼ˆå¯é€‰ï¼‰ï¼š
        å¦‚æœæä¾›äº† normï¼Œå¯¹æœ€ç»ˆè¾“å‡ºç‰¹å¾è¿›è¡Œå½’ä¸€åŒ–å¤„ç†ã€‚
    è¿”å›ç»“æœï¼š
        è¾“å‡ºä¸Šä¸‹æ–‡å¢å¼ºçš„ç‰¹å¾ï¼Œä¾›åç»­æ¨¡å—ä½¿ç”¨ï¼ˆå¦‚ Transformer çš„ Decoder æˆ–åˆ†ç±»å¤´ï¼‰ã€‚
"""
class TransformerEncoder(nn.Module):

    def __init__(self, encoder_layer, num_layers, norm=None):
        super().__init__()
        self.layers = _get_clones(encoder_layer, num_layers)
        self.num_layers = num_layers
        self.norm = norm

    def forward(self, src,
                mask: Optional[Tensor] = None,
                src_key_padding_mask: Optional[Tensor] = None,
                pos: Optional[Tensor] = None):
        output = src
        """
        éå†æ‰€æœ‰ç¼–ç å™¨å±‚ self.layersã€‚æ¯ä¸€å±‚è°ƒç”¨ TransformerEncoderLayer çš„å‰å‘ä¼ æ’­æ–¹æ³•
        æ¯å±‚å¤„ç†åè¿”å›ä¸Šä¸‹æ–‡å¢å¼ºçš„ç‰¹å¾ï¼Œèµ‹å€¼ç»™ outputã€‚
        """
        for layer in self.layers:
            output = layer(output, src_mask=mask,
                           src_key_padding_mask=src_key_padding_mask, pos=pos)

        if self.norm is not None:
            output = self.norm(output)

        return output

"""
TransformerDecoder æ˜¯ Transformer æ¨¡å‹çš„è§£ç å™¨éƒ¨åˆ†ï¼Œ
è´Ÿè´£å°†ç¼–ç å™¨ç”Ÿæˆçš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆmemoryï¼‰å’Œç›®æ ‡æŸ¥è¯¢å‘é‡ï¼ˆtgtï¼‰ç»“åˆèµ·æ¥ï¼Œç”Ÿæˆè§£ç ç»“æœï¼ˆoutputï¼‰ã€‚
è§£ç å™¨ç”±å¤šå±‚ TransformerDecoderLayer å †å è€Œæˆã€‚
"""
class TransformerDecoder(nn.Module):

    def __init__(self, decoder_layer, num_layers, norm=None, return_intermediate=False):
        super().__init__()
        self.layers = _get_clones(decoder_layer, num_layers)
        self.num_layers = num_layers
        self.norm = norm
        self.return_intermediate = return_intermediate

    def forward(self, tgt, memory,
                tgt_mask: Optional[Tensor] = None,
                memory_mask: Optional[Tensor] = None,
                tgt_key_padding_mask: Optional[Tensor] = None,
                memory_key_padding_mask: Optional[Tensor] = None,
                pos: Optional[Tensor] = None,
                query_pos: Optional[Tensor] = None):
        output = tgt

        intermediate = []


        for layer in self.layers:
            output = layer(output, memory, tgt_mask=tgt_mask,
                           memory_mask=memory_mask,
                           tgt_key_padding_mask=tgt_key_padding_mask,
                           memory_key_padding_mask=memory_key_padding_mask,
                           pos=pos, query_pos=query_pos)
            if self.return_intermediate:
                intermediate.append(self.norm(output))
        # å¯é€‰çš„å±‚å½’ä¸€åŒ–
        if self.norm is not None:
            output = self.norm(output)
            if self.return_intermediate:
                intermediate.pop()
                intermediate.append(output)

        if self.return_intermediate:
            return torch.stack(intermediate)

        return output.unsqueeze(0)

"""
è¿™ä¸ª TransformerEncoderLayer ç±»æ˜¯ Transformer æ¨¡å‹ä¸­çš„å•å±‚ç¼–ç å™¨æ¨¡å—ï¼ŒåŒ…å«ä»¥ä¸‹æ ¸å¿ƒç»„ä»¶ï¼š
å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼ˆSelf-Attentionï¼‰ã€‚
å‰é¦ˆç½‘ç»œï¼ˆFeedforward Network, FFNï¼‰ã€‚
å½’ä¸€åŒ–ï¼ˆLayer Normalizationï¼‰ã€‚
å¯é€‰çš„ Dropoutã€‚

å·¥ä½œæµç¨‹æ€»ç»“
å¤šå¤´è‡ªæ³¨æ„åŠ›ï¼š
    æ•è·åºåˆ—ä¸­å„ä½ç½®ä¹‹é—´çš„å…³ç³»ã€‚
    ç»“æœé€šè¿‡æ®‹å·®è¿æ¥è¿”å›è¾“å…¥ã€‚
å‰é¦ˆç½‘ç»œï¼š
    è¿›ä¸€æ­¥å¤„ç†æ³¨æ„åŠ›è¾“å‡ºï¼Œæå‡ç‰¹å¾è¡¨è¾¾èƒ½åŠ›ã€‚
å½’ä¸€åŒ–ï¼š
    é€šè¿‡ LayerNorm ç¨³å®šè®­ç»ƒè¿‡ç¨‹ã€‚
æ®‹å·®è¿æ¥ï¼š
    ä¿æŒæ¢¯åº¦æµåŠ¨ï¼Œé¿å…æ·±å±‚ç½‘ç»œçš„æ¢¯åº¦æ¶ˆå¤±ã€‚
"""
class TransformerEncoderLayer(nn.Module):

    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1,
                 activation="relu", normalize_before=False):
        super().__init__()
        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)
        # Implementation of Feedforward model
        self.linear1 = nn.Linear(d_model, dim_feedforward)
        self.dropout = nn.Dropout(dropout)
        self.linear2 = nn.Linear(dim_feedforward, d_model)

        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.dropout1 = nn.Dropout(dropout)
        self.dropout2 = nn.Dropout(dropout)

        self.activation = _get_activation_fn(activation)
        self.normalize_before = normalize_before

    """
    å¦‚æœä½ç½®ç¼–ç  pos å­˜åœ¨ï¼Œå°†å…¶ä¸è¾“å…¥å¼ é‡ç›¸åŠ ã€‚
    ä½ç½®ç¼–ç å¼•å…¥åºåˆ—ä¸­æ¯ä¸ªä½ç½®çš„æ˜¾å¼ä½ç½®ä¿¡æ¯ã€‚
    """
    def with_pos_embed(self, tensor, pos: Optional[Tensor]):
        return tensor if pos is None else tensor + pos

    # å‰å‘ä¼ æ’­
    def forward_post(self,
                     src,
                     src_mask: Optional[Tensor] = None,
                     src_key_padding_mask: Optional[Tensor] = None,
                     pos: Optional[Tensor] = None):
        q = k = self.with_pos_embed(src, pos)
        src2 = self.self_attn(q, k, value=src, attn_mask=src_mask,
                              key_padding_mask=src_key_padding_mask)[0]
        src = src + self.dropout1(src2)
        src = self.norm1(src)
        src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))
        src = src + self.dropout2(src2)
        src = self.norm2(src)
        return src
    
    """
    ä¸ forward_post çš„å”¯ä¸€åŒºåˆ«æ˜¯å½’ä¸€åŒ–çš„é¡ºåºã€‚
    åœ¨ forward_pre ä¸­ï¼Œå½’ä¸€åŒ–å‘ç”Ÿåœ¨å¤šå¤´è‡ªæ³¨æ„åŠ›å’Œå‰é¦ˆç½‘ç»œä¹‹å‰ã€‚
    """
    def forward_pre(self, src,
                    src_mask: Optional[Tensor] = None,
                    src_key_padding_mask: Optional[Tensor] = None,
                    pos: Optional[Tensor] = None):
        src2 = self.norm1(src)
        q = k = self.with_pos_embed(src2, pos)
        src2 = self.self_attn(q, k, value=src2, attn_mask=src_mask,
                              key_padding_mask=src_key_padding_mask)[0]
        src = src + self.dropout1(src2)
        src2 = self.norm2(src)
        src2 = self.linear2(self.dropout(self.activation(self.linear1(src2))))
        src = src + self.dropout2(src2)
        return src

    """
    æ ¹æ® self.normalize_before å‚æ•°é€‰æ‹©å½’ä¸€åŒ–æ–¹å¼ï¼š
    normalize_before=Trueï¼šä½¿ç”¨ forward_preï¼Œå½’ä¸€åŒ–åœ¨æ“ä½œä¹‹å‰æ‰§è¡Œã€‚
    normalize_before=Falseï¼šä½¿ç”¨ forward_postï¼Œå½’ä¸€åŒ–åœ¨æ“ä½œä¹‹åæ‰§è¡Œã€‚
    """
    def forward(self, src,
                src_mask: Optional[Tensor] = None,
                src_key_padding_mask: Optional[Tensor] = None,
                pos: Optional[Tensor] = None):
        if self.normalize_before:
            return self.forward_pre(src, src_mask, src_key_padding_mask, pos)
        return self.forward_post(src, src_mask, src_key_padding_mask, pos)

"""
TransformerDecoderLayer æ˜¯ Transformer è§£ç å™¨çš„æ ¸å¿ƒæ¨¡å—,å•å±‚decoderlayerï¼Œ
è´Ÿè´£å¯¹ç›®æ ‡åºåˆ—ï¼ˆtgtï¼‰è¿›è¡Œè§£ç ï¼Œç»“åˆç¼–ç å™¨çš„è¾“å‡ºï¼ˆmemoryï¼‰ç”Ÿæˆæœ€ç»ˆçš„è§£ç ç‰¹å¾ã€‚
æ ¸å¿ƒä½œç”¨ï¼š
    é€šè¿‡å¤šå¤´è‡ªæ³¨æ„åŠ›å»ºæ¨¡ç›®æ ‡åºåˆ—çš„ä¾èµ–å…³ç³»ã€‚
    é€šè¿‡äº¤å‰æ³¨æ„åŠ›ä»ç¼–ç å™¨è¾“å‡ºä¸­æå–ç›¸å…³ä¿¡æ¯ã€‚
    åˆ©ç”¨å‰é¦ˆç½‘ç»œè¿›ä¸€æ­¥å¤„ç†ç‰¹å¾ã€‚
    ä½¿ç”¨æ®‹å·®è¿æ¥å’Œå½’ä¸€åŒ–ç¨³å®šè®­ç»ƒã€‚

pos å’Œ query_posï¼šä½ç½®ç¼–ç ï¼Œåˆ†åˆ«ä¸ºç¼–ç å™¨è¾“å‡ºå’Œç›®æ ‡æŸ¥è¯¢çš„ä½ç½®ã€‚

å·¥ä½œæµç¨‹æ€»ç»“
    è‡ªæ³¨æ„åŠ›ï¼š
        å»ºæ¨¡ç›®æ ‡åºåˆ—ä¸­å„ä½ç½®çš„å…³ç³»ã€‚
    äº¤å‰æ³¨æ„åŠ›ï¼š
        ä»ç¼–ç å™¨è¾“å‡ºä¸­æå–ç›®æ ‡ç›¸å…³çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚
    å‰é¦ˆç½‘ç»œï¼š
        æå‡ç›®æ ‡ç‰¹å¾çš„è¡¨è¾¾èƒ½åŠ›ã€‚
    å½’ä¸€åŒ–å’Œæ®‹å·®è¿æ¥ï¼š
        æé«˜è®­ç»ƒç¨³å®šæ€§ï¼Œé˜²æ­¢æ¢¯åº¦æ¶ˆå¤±æˆ–çˆ†ç‚¸ã€‚
"""
class TransformerDecoderLayer(nn.Module):

    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1,
                 activation="relu", normalize_before=False):
        super().__init__()
        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)
        self.multihead_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)
        # Implementation of Feedforward model
        self.linear1 = nn.Linear(d_model, dim_feedforward)
        self.dropout = nn.Dropout(dropout)
        self.linear2 = nn.Linear(dim_feedforward, d_model)

        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.norm3 = nn.LayerNorm(d_model)
        self.dropout1 = nn.Dropout(dropout)
        self.dropout2 = nn.Dropout(dropout)
        self.dropout3 = nn.Dropout(dropout)

        self.activation = _get_activation_fn(activation)
        self.normalize_before = normalize_before

    def with_pos_embed(self, tensor, pos: Optional[Tensor]):
        return tensor if pos is None else tensor + pos

    def forward_post(self, tgt, memory,
                     tgt_mask: Optional[Tensor] = None,
                     memory_mask: Optional[Tensor] = None,
                     tgt_key_padding_mask: Optional[Tensor] = None,
                     memory_key_padding_mask: Optional[Tensor] = None,
                     pos: Optional[Tensor] = None,
                     query_pos: Optional[Tensor] = None):
        # ç›®æ ‡åºåˆ—çš„è‡ªæ³¨æ„åŠ›
        """
        ä½¿ç”¨ç›®æ ‡æŸ¥è¯¢ query_pos å’Œç›®æ ‡ç‰¹å¾ tgtï¼Œè®¡ç®—ç›®æ ‡åºåˆ—ä¸­ä½ç½®é—´çš„å…³ç³»ã€‚
        é€šè¿‡æ®‹å·®è¿æ¥å’Œå½’ä¸€åŒ–ç¨³å®šè®­ç»ƒã€‚
        """
        q = k = self.with_pos_embed(tgt, query_pos)
        tgt2 = self.self_attn(q, k, value=tgt, attn_mask=tgt_mask,
                              key_padding_mask=tgt_key_padding_mask)[0]
        tgt = tgt + self.dropout1(tgt2)
        tgt = self.norm1(tgt)
        # äº¤å‰æ³¨æ„åŠ›
        """
        ä½¿ç”¨ç¼–ç å™¨çš„è¾“å‡º memory å’Œç›®æ ‡æŸ¥è¯¢ï¼Œæå–ç›®æ ‡ç›¸å…³çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚
        é€šè¿‡æ®‹å·®è¿æ¥å’Œå½’ä¸€åŒ–ï¼Œå¢å¼ºç‰¹å¾çš„ç¨³å®šæ€§ã€‚
        """
        tgt2 = self.multihead_attn(query=self.with_pos_embed(tgt, query_pos),
                                   key=self.with_pos_embed(memory, pos),
                                   value=memory, attn_mask=memory_mask,
                                   key_padding_mask=memory_key_padding_mask)[0]
        tgt = tgt + self.dropout2(tgt2)
        tgt = self.norm2(tgt)
        """
        å¯¹ç›®æ ‡ç‰¹å¾é€šè¿‡å‰é¦ˆç½‘ç»œè¿›è¡Œè¿›ä¸€æ­¥å¤„ç†ï¼Œæå‡ç‰¹å¾è¡¨è¾¾èƒ½åŠ›ã€‚
        æœ€åé€šè¿‡æ®‹å·®è¿æ¥å’Œå½’ä¸€åŒ–è¿”å›ç»“æœã€‚
        """
        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))
        tgt = tgt + self.dropout3(tgt2)
        tgt = self.norm3(tgt)
        return tgt

    def forward_pre(self, tgt, memory,
                    tgt_mask: Optional[Tensor] = None,
                    memory_mask: Optional[Tensor] = None,
                    tgt_key_padding_mask: Optional[Tensor] = None,
                    memory_key_padding_mask: Optional[Tensor] = None,
                    pos: Optional[Tensor] = None,
                    query_pos: Optional[Tensor] = None):
        tgt2 = self.norm1(tgt)
        q = k = self.with_pos_embed(tgt2, query_pos)
        tgt2 = self.self_attn(q, k, value=tgt2, attn_mask=tgt_mask,
                              key_padding_mask=tgt_key_padding_mask)[0]
        tgt = tgt + self.dropout1(tgt2)
        tgt2 = self.norm2(tgt)
        tgt2 = self.multihead_attn(query=self.with_pos_embed(tgt2, query_pos),
                                   key=self.with_pos_embed(memory, pos),
                                   value=memory, attn_mask=memory_mask,
                                   key_padding_mask=memory_key_padding_mask)[0]
        tgt = tgt + self.dropout2(tgt2)
        tgt2 = self.norm3(tgt)
        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt2))))
        tgt = tgt + self.dropout3(tgt2)
        return tgt

    def forward(self, tgt, memory,
                tgt_mask: Optional[Tensor] = None,
                memory_mask: Optional[Tensor] = None,
                tgt_key_padding_mask: Optional[Tensor] = None,
                memory_key_padding_mask: Optional[Tensor] = None,
                pos: Optional[Tensor] = None,
                query_pos: Optional[Tensor] = None):
        if self.normalize_before:
            return self.forward_pre(tgt, memory, tgt_mask, memory_mask,
                                    tgt_key_padding_mask, memory_key_padding_mask, pos, query_pos)
        return self.forward_post(tgt, memory, tgt_mask, memory_mask,
                                 tgt_key_padding_mask, memory_key_padding_mask, pos, query_pos)


def _get_clones(module, N):
    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])


def build_transformer(args):
    return Transformer(
        d_model=args.hidden_dim,
        dropout=args.dropout,
        nhead=args.nheads,
        dim_feedforward=args.dim_feedforward,
        num_encoder_layers=args.enc_layers,
        num_decoder_layers=args.dec_layers,
        normalize_before=args.pre_norm,
        return_intermediate_dec=True,
    )


def _get_activation_fn(activation):
    """Return an activation function given a string"""
    if activation == "relu":
        return F.relu
    if activation == "gelu":
        return F.gelu
    if activation == "glu":
        return F.glu
    raise RuntimeError(F"activation should be relu/gelu, not {activation}.")
